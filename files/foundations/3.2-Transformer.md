### 3.2 æ·±åº¦å­¦ä¹ æ‰“åº•ï¼šSelf-Attentionä¸Transformer

#### èµ·æ­¥ä¸‰ä»¶äº‹

â­ **å¿…çœ‹**ï¼š[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/ ) - Jay Alammarçš„å¯è§†åŒ–è®²è§£

ğŸ§ª **å®ä½œ**ï¼šæ‰‹æ’•ä¸€ä¸ªmini Transformerï¼ˆ<500è¡Œä»£ç ï¼‰

ğŸ“„ **è®ºæ–‡**ï¼š[Attention Is All You Need](https://arxiv.org/abs/1706.03762 ) - åŸå§‹è®ºæ–‡

#### Transformerï¼šä»é»‘ç›’åˆ°æŒæ¡å…¨è²Œ

å’±å…ˆä»æœ€ç®€å•çš„è§†è§’å¼€å§‹â€”â€”æŠŠ Transformer å½“æˆä¸€ä¸ªé»‘ç›’å­ã€‚

**ç¬¬ä¸€å±‚ç†è§£ï¼šçº¯é»‘ç›’**

![è¾“å…¥è¾“å‡ºé»‘ç›’](../images/ç¬¬ä¸‰èŠ‚/2-input-output.png)

å°±è¿™ä¹ˆç®€å•ï¼Œæ³•è¯­è¿›å»ï¼Œè‹±è¯­å‡ºæ¥ã€‚åˆ«ç®¡é‡Œé¢å’‹å®ç°çš„ï¼Œåæ­£å®ƒèƒ½å·¥ä½œã€‚è¿™å°±æ˜¯2017å¹´Googleå‘è¡¨ã€ŠAttention Is All You Needã€‹æ—¶éœ‡æ’¼ä¸–ç•Œçš„åŸå› â€”â€”è¿™ç©æ„å„¿æŠŠæœºå™¨ç¿»è¯‘çš„è´¨é‡æå‡åˆ°äº†å‰æ‰€æœªæœ‰çš„é«˜åº¦ã€‚

**ç¬¬äºŒå±‚ç†è§£ï¼šæ‰“å¼€ç›–å­çœ‹å¤§ç»“æ„**

![ç¼–ç å™¨è§£ç å™¨ç»“æ„](../images/ç¬¬ä¸‰èŠ‚/2-encoder-decoder.png)

æ€å¼€ç›–å­ï¼Œé‡Œé¢æ˜¯ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤å¤§å—ã€‚ç¼–ç å™¨è´Ÿè´£ç†è§£è¾“å…¥ï¼Œè§£ç å™¨è´Ÿè´£ç”Ÿæˆè¾“å‡ºã€‚åŸè®ºæ–‡å„ç”¨ 6 å±‚ï¼Œä½†è¿™ä¸æ˜¯æ­»çš„â€”â€”BERT ç”¨ 12 å±‚ï¼ŒGPT-3 æ›´å¤¸å¼ ç”¨äº† 96 å±‚ï¼

**ç¬¬ä¸‰å±‚ç†è§£ï¼šå•å±‚å†…éƒ¨åˆ°åº•å¹²å•¥**

![å•å±‚ç¼–ç å™¨ç»“æ„](../images/ç¬¬ä¸‰èŠ‚/2-encoder.png)

æ¯ä¸€å±‚ç¼–ç å™¨å°±å¹²ä¸¤ä»¶äº‹ï¼š
1. **Self-Attention**ï¼šè®©æ¯ä¸ªè¯éƒ½èƒ½"çœ‹åˆ°"æ•´ä¸ªå¥å­çš„ä¿¡æ¯
2. **Feed Forward**ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åšä¸€æ¬¡éçº¿æ€§å˜æ¢

è¿™ä¸¤æ­¥éƒ½é…äº†"è·³çº¿"ï¼ˆæ®‹å·®è¿æ¥ï¼‰å’Œ"å½’ä¸€åŒ–"ï¼ˆLayer Normï¼‰ï¼Œä¿è¯ä¿¡å·ç¨³å®šä¼ é€’ã€‚

![Transformeræ•´ä½“æ¶æ„](../images/ç¬¬ä¸‰èŠ‚/transformer%20structure.png)

#### Self-Attentionï¼šæ ¸å¿ƒä¸­çš„æ ¸å¿ƒ

**ä¸ºå•¥éœ€è¦Self-Attentionï¼Ÿ**

æƒ³è±¡ä½ åœ¨è¯»è¿™å¥è¯ï¼š"å°æ˜å–œæ¬¢æ‰“ç¯®çƒï¼Œä»–æ¯å¤©éƒ½å»çƒåœº"ã€‚å½“æ¨¡å‹å¤„ç†"ä»–"è¿™ä¸ªè¯æ—¶ï¼Œéœ€è¦çŸ¥é“"ä»–"æŒ‡çš„æ˜¯"å°æ˜"ã€‚ä¼ ç»Ÿ RNN å¾—ä¸€ä¸ªè¯ä¸€ä¸ªè¯å¾€å‰ä¼ ï¼Œä¼ åˆ°"ä»–"çš„æ—¶å€™"å°æ˜"çš„ä¿¡æ¯å¯èƒ½å·²ç»ä¸¢å¤±äº†ã€‚CNN åªèƒ½çœ‹å›ºå®šå¤§å°çš„çª—å£ã€‚

Self-Attention çš„é©å‘½æ€§åœ¨äºï¼š**è®©åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥"å¯¹è¯"**ï¼

**Query-Key-Value æœºåˆ¶**

![QKVæœºåˆ¶](../images/ç¬¬ä¸‰èŠ‚/2-qkv.png)

è¿™ä¸ªæœºåˆ¶ç‰¹åˆ«åƒåœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦ï¼š
- **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šä½ æƒ³æ‰¾ä»€ä¹ˆä¹¦ï¼ˆ"æˆ‘éœ€è¦å…³äºæœºå™¨å­¦ä¹ çš„èµ„æ–™"ï¼‰
- **Keyï¼ˆé”®ï¼‰**ï¼šæ¯æœ¬ä¹¦çš„æ ‡ç­¾ï¼ˆ"æ·±åº¦å­¦ä¹ "ã€"ç»Ÿè®¡å­¦ä¹ "ã€"ç¥ç»ç½‘ç»œ"ï¼‰
- **Valueï¼ˆå€¼ï¼‰**ï¼šä¹¦çš„å®é™…å†…å®¹

æ•°å­¦ä¸Šå°±æ˜¯ï¼š
```python
# å‡è®¾è¾“å…¥åºåˆ— X å½¢çŠ¶ä¸º [batch, seq_len, d_model]
Q = X @ W_Q  # ç”ŸæˆæŸ¥è¯¢
K = X @ W_K  # ç”Ÿæˆé”®
V = X @ W_V  # ç”Ÿæˆå€¼

# è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
scores = Q @ K.T / sqrt(d_k)  # ä¸ºå•¥é™¤ä»¥æ ¹å·dï¼Ÿé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
attention_weights = softmax(scores)
output = attention_weights @ V
```

ä¸ºä»€ä¹ˆè¦é™¤ä»¥ $\sqrt{d_k}$ï¼Ÿè‹å‰‘æ—åœ¨[ã€Šæµ…è°ˆTransformerçš„åˆå§‹åŒ–ã€å‚æ•°åŒ–ä¸æ ‡å‡†åŒ–ã€‹](https://zhuanlan.zhihu.com/p/400925524 )é‡Œè®²å¾—ç‰¹åˆ«æ¸…æ¥šâ€”â€”ç‚¹ç§¯çš„æ–¹å·®ä¼šéšç»´åº¦å¢é•¿ï¼Œä¸é™¤çš„è¯softmaxä¼šé¥±å’Œã€‚

**æ³¨æ„åŠ›å¯è§†åŒ–**

![è¯æ³¨æ„åŠ›å¯è§†åŒ–](../images/ç¬¬ä¸‰èŠ‚/2-attention-word.png)

è¿™å›¾å±•ç¤ºäº†æ¨¡å‹åœ¨ç¿»è¯‘æ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚æ¯”å¦‚ç¿»è¯‘"The animal didn't cross the street because it was too tired"æ—¶ï¼Œæ¨¡å‹éœ€è¦åˆ¤æ–­"it"æŒ‡ä»£ä»€ä¹ˆã€‚é€šè¿‡æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°æ¨¡å‹ç¡®å®å­¦ä¼šäº†æ­£ç¡®çš„æŒ‡ä»£å…³ç³»ã€‚

#### å¤šå¤´æ³¨æ„åŠ›ï¼šå›¢é˜Ÿåä½œ

![å¤šå¤´æ³¨æ„åŠ›](../images/ç¬¬ä¸‰èŠ‚/2-multi-head.png)

å•ä¸ªæ³¨æ„åŠ›å¤´å°±åƒä¸€ä¸ªä¸“å®¶ï¼Œå¤šå¤´æ³¨æ„åŠ›å°±æ˜¯ä¸“å®¶å›¢é˜Ÿã€‚å‡è®¾ d_model=512ï¼Œç”¨ 8 ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´è´Ÿè´£ 64 ç»´ï¼š

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, n_heads=8):
        super().__init__()
        self.d_k = d_model // n_heads  # 64
        self.n_heads = n_heads

        # ä¸ºæ‰€æœ‰å¤´ä¸€èµ·è®¡ç®—QKV
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # 1. è®¡ç®—QKVå¹¶reshapeæˆå¤šå¤´
        Q = self.W_Q(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        K = self.W_K(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        V = self.W_V(x).view(batch_size, seq_len, self.n_heads, self.d_k)

        # 2. è½¬ç½®ä¾¿äºå¹¶è¡Œè®¡ç®—: [batch, n_heads, seq_len, d_k]
        Q = Q.transpose(1, 2)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores.masked_fill_(mask == 0, -1e9)

        attention = F.softmax(scores, dim=-1)
        context = torch.matmul(attention, V)

        # 4. åˆå¹¶å¤šå¤´
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, seq_len, d_model)

        return self.W_O(context)
```

ä¸åŒçš„å¤´ä¼šå­¦åˆ°ä¸åŒç±»å‹çš„å…³ç³»ï¼š
- **å¤´1**ï¼šå¯èƒ½ä¸“æ³¨äºå±€éƒ¨è¯­æ³•ï¼ˆç›¸é‚»è¯å…³ç³»ï¼‰
- **å¤´2**ï¼šå¯èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼ˆä¸»è¯­-è°“è¯­ï¼‰
- **å¤´3**ï¼šå¯èƒ½å­¦ä¹ æŒ‡ä»£å…³ç³»ï¼ˆä»£è¯-åè¯ï¼‰
- **å¤´4-8**ï¼šå„æœ‰åˆ†å·¥ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦ã€å¥æ³•ç»“æ„ç­‰

#### ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡å‹è¯åº

![ä½ç½®ç¼–ç ](../images/ç¬¬ä¸‰èŠ‚/2-position.png)

Self-Attention æœ‰ä¸ªé—®é¢˜â€”â€”å®ƒä¸çŸ¥é“è¯çš„é¡ºåºï¼æ‰€ä»¥éœ€è¦ä½ç½®ç¼–ç ï¼š

![ä½ç½®ç¼–ç å¯è§†åŒ–](../images/ç¬¬ä¸‰èŠ‚/2-2-pos-embedding.png)

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()

        # åˆ›å»ºé¢‘ç‡é¡¹
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        # å¶æ•°ç»´åº¦ç”¨sin
        pe[:, 0::2] = torch.sin(position * div_term)
        # å¥‡æ•°ç»´åº¦ç”¨cos
        pe[:, 1::2] = torch.cos(position * div_term)

        # æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸è®­ç»ƒ
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        return x + self.pe[:, :x.size(1)]
```

ä¸ºå•¥ç”¨ sin/cosï¼Ÿå› ä¸ºå®ƒä»¬æœ‰ä¸ªç¥å¥‡çš„æ€§è´¨ï¼š
```
PE(pos+k) å¯ä»¥è¡¨ç¤ºä¸º PE(pos) å’Œ PE(k) çš„çº¿æ€§ç»„åˆ
```
è¿™è®©æ¨¡å‹èƒ½å­¦åˆ°ç›¸å¯¹ä½ç½®å…³ç³»ï¼

#### æ®‹å·®è¿æ¥ä¸Layer Norm

![æ®‹å·®è¿æ¥](../images/ç¬¬ä¸‰èŠ‚/2-resnet.png)

æ¯ä¸ªå­å±‚éƒ½ç”¨äº†æ®‹å·®è¿æ¥ï¼ˆå€Ÿé‰´ResNetï¼‰+ Layer Normalizationï¼š

```python
class ResidualConnection(nn.Module):
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        # Post-LN: x + dropout(sublayer(norm(x)))
        return x + self.dropout(sublayer(self.norm(x)))
```

ä¸ºå•¥è¦è¿™ä¹ˆåšï¼Ÿ
- **æ®‹å·®è¿æ¥**ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œè®©æ·±å±‚ç½‘ç»œè®­ç»ƒæ›´ç¨³å®š
- **Layer Norm**ï¼šç¨³å®šæ¯å±‚çš„è¾“å…¥åˆ†å¸ƒï¼ŒåŠ é€Ÿè®­ç»ƒ

#### Encoderå®Œæ•´å®ç°

![ç¼–ç å™¨æµç¨‹](../images/ç¬¬ä¸‰èŠ‚/2-x-encoder.png)

æŠŠæ‰€æœ‰ç»„ä»¶æ‹¼èµ·æ¥ï¼š

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        self.residual1 = ResidualConnection(d_model, dropout)
        self.residual2 = ResidualConnection(d_model, dropout)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®
        x = self.residual1(x, lambda x: self.attention(x, mask))
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®
        x = self.residual2(x, self.feed_forward)
        return x

class Encoder(nn.Module):
    def __init__(self, n_layers=6, d_model=512, n_heads=8):
        super().__init__()
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads) for _ in range(n_layers)
        ])

    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask)
        return x
```

#### Decoderï¼šå¸¦Maskçš„ç”Ÿæˆå™¨

è§£ç å™¨æ¯”ç¼–ç å™¨å¤šäº†ä¸¤ä¸ªä¸œè¥¿ï¼š
1. **Masked Self-Attention**ï¼šç”Ÿæˆæ—¶ä¸èƒ½çœ‹åˆ°æœªæ¥çš„è¯
2. **Encoder-Decoder Attention**ï¼šæ¥æ”¶ç¼–ç å™¨çš„è¾“å‡ºä½œä¸º K å’Œ V

```python
def create_look_ahead_mask(seq_len):
    """åˆ›å»ºä¸‹ä¸‰è§’æ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]

# ä½¿ç”¨ç¤ºä¾‹
mask = create_look_ahead_mask(10)
# mask[0, 0, 3, :] = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
# ä½ç½®3åªèƒ½çœ‹åˆ°ä½ç½®0,1,2,3
```

#### å®æˆ˜è¸©å‘ç»éªŒ

**å‘1ï¼šç»´åº¦å¯¹ä¸ä¸Š**
```python
# âŒ æ–°æ‰‹å¸¸çŠ¯é”™è¯¯
Q = self.W_Q(x)  # [batch, seq, d_model]
K = self.W_K(x)  # [batch, seq, d_model]
attention = torch.bmm(Q, K.T)  # æŠ¥é”™ï¼K.Tä¸å¯¹

# âœ… æ­£ç¡®åšæ³•
attention = torch.matmul(Q, K.transpose(-2, -1))
# æˆ–è€…
attention = Q @ K.transpose(-2, -1)
```

**å‘2ï¼šå¿˜è®°ç¼©æ”¾**
```python
# âŒ ä¸ç¼©æ”¾ï¼Œæ¢¯åº¦å®¹æ˜“çˆ†ç‚¸
scores = Q @ K.transpose(-2, -1)

# âœ… é™¤ä»¥æ ¹å·d_kç¨³å®šè®­ç»ƒ
scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)
```

**å‘3ï¼šä½ç½®ç¼–ç å½“å‚æ•°è®­ç»ƒ**
```python
# âŒ ä½ç½®ç¼–ç ä¸åº”è¯¥è¢«è®­ç»ƒ
self.pos_encoding = nn.Parameter(torch.randn(max_len, d_model))

# âœ… ç”¨register_bufferå›ºå®š
self.register_buffer('pos_encoding', create_positional_encoding(max_len, d_model))
```

**å‘4ï¼šLayer Norm ä½ç½®**
```python
# Post-LNï¼ˆåŸè®ºæ–‡ï¼‰
x = x + sublayer(layer_norm(x))

# Pre-LNï¼ˆæ›´ç¨³å®šï¼‰
x = x + sublayer(x)
x = layer_norm(x)
```

#### ä¸ºä»€ä¹ˆ Transformer è¿™ä¹ˆå¼ºï¼Ÿ

**å¹¶è¡Œæ€§**
- RNN å¿…é¡»ä¸²è¡Œå¤„ç†ï¼Œç¬¬ t æ­¥ä¾èµ–ç¬¬ t-1 æ­¥
- Transformer å¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—ï¼Œè®­ç»ƒé€Ÿåº¦é£å¿«

**é•¿è·ç¦»ä¾èµ–**
- RNN ä¿¡æ¯ä¼ é€’è·¯å¾„é•¿ï¼Œå®¹æ˜“é—å¿˜
- Transformer ä»»æ„ä¸¤ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥äº¤äº’

**è¡¨è¾¾èƒ½åŠ›**
- å¤šå¤´æ³¨æ„åŠ› = å¤šä¸ªç‰¹å¾æå–å™¨å¹¶è¡Œå·¥ä½œ
- æ¯ä¸ªå¤´å¯ä»¥å­¦ä¹ ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»

**è®¡ç®—å¤æ‚åº¦å¯¹æ¯”**
| æ¨¡å‹ | æ¯å±‚å¤æ‚åº¦ | å¹¶è¡Œåº¦ | æœ€å¤§è·¯å¾„é•¿åº¦ |
|------|------------|--------|--------------|
| RNN | O(nÂ·dÂ²) | O(n) | O(n) |
| CNN | O(kÂ·nÂ·dÂ²) | O(1) | O(log_k(n)) |
| Transformer | O(nÂ²Â·d) | O(1) | O(1) |

è™½ç„¶æ˜¯ O(nÂ²)ï¼Œä½†å¯¹äºå¸¸è§é•¿åº¦ï¼ˆ<512ï¼‰ï¼Œè¿™ä¸æ˜¯ç“¶é¢ˆã€‚ç“¶é¢ˆé€šå¸¸åœ¨ dï¼ˆæ¨¡å‹ç»´åº¦ï¼‰ä¸Šã€‚

#### è°ƒè¯•æŠ€å·§æ€»ç»“

1. **å…ˆè·‘é€šå°æ¨¡å‹**ï¼š2 å±‚ã€128 ç»´åº¦ã€4 ä¸ªå¤´ï¼Œç¡®ä¿æµç¨‹æ­£ç¡®
2. **å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡**ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­¦åˆ°åˆç†çš„æ¨¡å¼
3. **æ¢¯åº¦è£å‰ª**ï¼š`torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)`
4. **å­¦ä¹ ç‡warmupå¿…é¡»æœ‰**ï¼šå‰ 4000 æ­¥çº¿æ€§å¢é•¿å¾ˆé‡è¦
5. **æ£€æŸ¥maskæ˜¯å¦æ­£ç¡®**ï¼šæ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œå¾ˆå¤š bug åœ¨è¿™