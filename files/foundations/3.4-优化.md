### 3.3 深度学习打底：优化/正则化/训练技巧

#### 让模型真正学起来：优化器的选择

训练神经网络就像爬山找最低点，优化器决定了你怎么走。

**最简单的SGD（随机梯度下降）**

```python
# 最朴素的更新方式
for batch in dataloader:
    loss = model(batch)
    loss.backward()  # 算梯度

    # 手动更新参数（实际用optimizer.step()）
    for param in model.parameters():
        param.data -= learning_rate * param.grad
```

问题来了——SGD 太慢了！就像你走迷宫，每一步都小心翼翼，遇到小坑就卡住了。

**带动量的SGD：像滚雪球**

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
```

动量是啥？想象你推一个球下山：
- 没动量：球在每个小坑都会停
- 有动量：球有惯性，能冲过小坑继续滚

实际效果对比：
```python
# 普通SGD训练ResNet50，100个epoch才收敛
# 加了momentum=0.9，60个epoch就够了
```

**Adam：懒人首选**

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

Adam 聪明在哪？
1. 自适应学习率——每个参数都有自己的学习率
2. 结合了动量——跑得快
3. 几乎不用调参——设lr=1e-3基本就行

具体原理（看不懂可以跳过）：
```python
# Adam内部大概这么算的
class SimpleAdam:
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999)):
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.m = {}  # 一阶矩（动量）
        self.v = {}  # 二阶矩（梯度平方的滑动平均）
        self.t = 0   # 时间步

    def step(self):
        self.t += 1
        for param in self.params:
            grad = param.grad

            # 更新一阶矩和二阶矩
            self.m[param] = self.beta1 * self.m.get(param, 0) + (1 - self.beta1) * grad
            self.v[param] = self.beta2 * self.v.get(param, 0) + (1 - self.beta2) * grad**2

            # 偏差修正（刚开始时m和v都接近0，需要修正）
            m_hat = self.m[param] / (1 - self.beta1**self.t)
            v_hat = self.v[param] / (1 - self.beta2**self.t)

            # 更新参数
            param.data -= self.lr * m_hat / (v_hat**0.5 + 1e-8)
```

**什么时候用什么优化器？**

做 CV（计算机视觉）：
```python
# ResNet、VGG这种，SGD+Momentum效果好
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
# 但是要配合学习率衰减，不然后期会震荡
```

做 NLP 或 Transformer：
```python
# Adam几乎是标配
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
# BERT微调时学习率要小
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
```

做强化学习：
```python
# PPO这种用Adam，学习率更小
optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)
```

#### 学习率调度：训练的节奏感

学习率不能一成不变。就像学车，刚开始大步调整，熟练后要精细控制。

**最常用的几种调度器**

1. **阶梯式下降**（训练 CNN 常用）
```python
# 每30个epoch，学习率乘以0.1
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# 训练循环
for epoch in range(100):
    train_one_epoch()
    scheduler.step()  # 别忘了调用！

    # 实际的学习率变化：
    # epoch 0-29:  lr = 0.1
    # epoch 30-59: lr = 0.01
    # epoch 60-89: lr = 0.001
```

2. **余弦退火**（比阶梯更平滑）
```python
# T_max是半个余弦周期
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# 学习率像余弦函数一样平滑下降
# 不会突然掉cliff，训练更稳定
```

3. **Warmup**（Transformer 必备）
```python
# Transformer论文的warmup公式
class WarmupScheduler:
    def __init__(self, optimizer, d_model, warmup_steps):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0

    def step(self):
        self.step_num += 1
        # 先线性增长，后按step^-0.5下降
        lr = self.d_model**(-0.5) * min(
            self.step_num**(-0.5),
            self.step_num * self.warmup_steps**(-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# 为啥需要warmup？
# 刚开始模型参数是随机的，梯度很大
# 如果学习率也大，容易飞掉
# 所以先用小学习率"热身"
```

实际使用时，可以用 transformers 库：
```python
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,  # 前1000步warmup
    num_training_steps=10000  # 总共10000步
)
```

#### 防止过拟合：正则化技术

模型太强也不好，会把训练集的噪声都记住。就像考试，死记硬背每道题答案，换个题就不会了。

**Dropout：随机关闭神经元**

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout1 = nn.Dropout(0.5)  # 50%概率关闭
        self.fc2 = nn.Linear(256, 10)
        self.dropout2 = nn.Dropout(0.2)  # 20%概率关闭

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)  # 训练时随机置零，测试时不变
        x = self.fc2(x)
        x = self.dropout2(x)
        return x
```

不同场景 dropout 设多少？
- 全连接层：0.5 是经典值（Hinton 论文）
- CNN 卷积层：0.1-0.2（卷积层本身就有正则化效果）
- Transformer：0.1（位置都差不多）
- 太大会欠拟合，太小没效果

**BatchNorm vs LayerNorm**

这俩都是归一化，但归一化的维度不同：

```python
# 假设输入 x 的形状是 [batch_size=32, seq_len=100, hidden=512]

# BatchNorm: 对batch维度归一化
bn = nn.BatchNorm1d(512)
# 它会计算这32个样本在每个特征上的均值和方差

# LayerNorm: 对特征维度归一化
ln = nn.LayerNorm(512)
# 它会计算每个样本512个特征的均值和方差

# 直观理解：
# BatchNorm: "这批人的平均身高是多少？"
# LayerNorm: "这个人的各项指标平均是多少？"
```

为啥 Transformer 用 LayerNorm 不用 BatchNorm？
1. BatchNorm 依赖 batch size，batch 小了不稳定
2. 序列长度不固定时 BatchNorm 很麻烦
3. LayerNorm 对每个样本独立计算，更稳定

**Label Smoothing：让模型别太自信**

正常的分类标签是one-hot：
```python
# 假设3分类，真实标签是类别1
label = [0, 1, 0]  # 100%确信是类别1
```

Label smoothing 把它变成：
```python
# smooth_factor = 0.1
label = [0.05, 0.9, 0.05]  # 90%确信是类别1，各留5%可能

class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.classes = classes

    def forward(self, pred, target):
        # pred: [batch, classes]的logits
        # target: [batch]的类别索引

        # 创建smooth的标签
        smooth_label = torch.full_like(pred, self.smoothing / (self.classes - 1))
        smooth_label.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)

        # 用KL散度作为损失
        return F.kl_div(F.log_softmax(pred, dim=1), smooth_label, reduction='sum')
```

效果：模型不会过度自信，泛化性更好。特别是数据有标注噪声时很有用。

#### 混合精度训练：又快又省显存

正常训练用 float32（32位浮点数），混合精度用 float16（16位），显存省一半，速度快 2-3 倍！

```python
from torch.cuda.amp import autocast, GradScaler

# 创建梯度缩放器（防止fp16下溢）
scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    # 自动混合精度
    with autocast():
        outputs = model(batch['input'])  # 前向用fp16
        loss = criterion(outputs, batch['target'])

    # 反向传播
    scaler.scale(loss).backward()  # 梯度缩放
    scaler.step(optimizer)  # 更新参数
    scaler.update()  # 更新缩放器

# 注意事项：
# 1. 只在GPU上有用，CPU不支持
# 2. 老GPU（比如1080Ti）不支持，需要Volta架构以上（V100, 2080, 3090等）
# 3. 有些操作不稳定，比如BatchNorm有时会有问题
```

什么时候用混合精度？
- 模型很大，显存不够：必须用
- 想加速训练：推荐用
- 做实验调参：可以不用（先保证正确性）

混合精度的坑：
```python
# ❌ 梯度可能下溢变成0
loss.backward()

# ✅ 用GradScaler防止下溢
scaler.scale(loss).backward()

# ❌ 某些loss在fp16下不稳定
mae_loss = torch.mean(torch.abs(pred - target))

# ✅ 手动指定用fp32
with autocast():
    features = model(x)  # fp16
    with torch.cuda.amp.autocast(enabled=False):
        loss = custom_loss(features.float(), target.float())  # fp32
```